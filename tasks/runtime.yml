---
- name: Fail if openhpc_slurm_control_host or openhpc_cluster_name or openhpc_slurm_partitions are undefined
  fail:
    msg: "Undefined openhpc_slurm_control_host or openhpc_cluster_name or openhpc_slurm_partitions, or latter is empty."
  when:
    openhpc_slurm_control_host == none or
    openhpc_cluster_name == none or
    openhpc_slurm_partitions | length == 0

- name: Fail if configless mode selected when not on Centos 8
  fail:
    msg: "openhpc_slurm_configless = True requires Centos8 / OpenHPC v2"
  when: openhpc_slurm_configless and not ansible_distribution_major_version == "8"

- name: Ensure the Slurm spool directory exists
  file:
    path: /var/spool/slurm
    owner: slurm
    group: slurm
    mode: 0755
    state: directory

# Munge key handling is slightly complex as trying to cover cases with the following in the play:
# a. entire cluster (standard ansible deployment)
# b. only control node (part of image-based deploy)
# c. only single compute node (for image building)
# Therefore cannot depend on being able to generate a key on the control node and slurp + redistribute.
# A local key can be generated from the path {{ openhpc_munge_key }}, 

- name: Write Munge key from local disk
  copy:
    src: "{{ openhpc_munge_key }}"
    dest: "/etc/munge/munge.key"
    owner: munge
    group: munge
    mode: 0400
  when: openhpc_munge_key is defined
  notify:
    - Restart Munge service

- name: Generate a Munge key
  # NB this is usually a no-op as the package install actually generates a (node-unique) one, so won't usually trigger handler
  command: "dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024"
  args:
    creates: "/etc/munge/munge.key"
  notify:
    - Restart Munge service

- name: Retrieve Munge key
  slurp:
    src: "/etc/munge/munge.key"
  register: slurm_munge_key

- name: Save Munge key locally if single node play
  fetch:
    src: "/etc/munge/munge.key"
    dest: '.'
  register: output_munge_key
  when: play_hosts | length == 1
- debug:
    msg: "Generated munge key written to: {{ output_munge_key.dest }}"
  when: play_hosts | length == 1

- name: Write Munge key from control host, if available
  copy:
    content: "{{ hostvars[openhpc_slurm_control_host]['slurm_munge_key']['content'] | b64decode }}"
    dest: "/etc/munge/munge.key"
    owner: munge
    group: munge
    mode: 0400
  when: "hostvars[openhpc_slurm_control_host]['slurm_munge_key'] is defined"
  notify:
    - Restart Munge service

- name: Template slurmdbd.conf
  template:
    src: slurmdbd.conf.j2
    dest: /etc/slurm/slurmdbd.conf
    mode: "0600"
    owner: root
    group: root
  notify: Restart slurmdbd service
  when: openhpc_enable.database | default(false) | bool

- name: Apply customised SLURM configuration
  template:
    src: slurm.conf.j2
    dest: /etc/slurm/slurm.conf
    owner: root
    group: root
    mode: 0644
    lstrip_blocks: true
  when: openhpc_enable.control | default(false) or not openhpc_slurm_configless
  notify:
    - Reload SLURM service

- name: Set slurmctld location for configless operation
  lineinfile:
    path: /etc/sysconfig/slurmd
    line: "SLURMD_OPTIONS='--conf-server {{ openhpc_slurm_control_host }}'"
    regexp: "^SLURMD_OPTIONS="
    create: yes
    owner: root
    group: root
    mode: 0644
  when:
    - openhpc_enable.batch | default(false) | bool
    - openhpc_slurm_configless
  notify:
    - Reload SLURM service

# Munge state could be unchanged but the service is not running.
# Handle that here.
- name: Configure Munge service
  service:
    name: munge
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"

- name: Ensure slurmdbd is started and running
  service:
    name: slurmdbd
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
  when: openhpc_enable.database | default(false) | bool

# In case the service isn't running and the config hasn't changed to trigger
# the handler, ensure it's running here.
- name: Configure Slurm service
  service:
    name: "{{ openhpc_slurm_service }}"
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
  when: openhpc_slurm_service is not none
