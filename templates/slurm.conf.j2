#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ openhpc_cluster_name }}
ControlMachine={{ openhpc_slurm_control_host }}
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm
SlurmdSpoolDir=/var/spool/slurm
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
SelectType=select/cons_res
SelectTypeParameters=CR_Core
#FastSchedule=0
PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
PriorityWeightPartition=1000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
PreemptType=preempt/partition_prio
PreemptMode=SUSPEND,GANG
#
# LOGGING
# Silence warnings about configuration file mismatches.
DebugFlags=NO_CONF_HASH
SlurmctldDebug=3
#SlurmctldLogFile=
SlurmdDebug=3
#SlurmdLogFile=
JobCompType={{ openhpc_slurm_job_comp_type }}
JobCompLoc={{ openhpc_slurm_job_comp_loc }}
#
# ACCOUNTING
JobAcctGatherType={{ openhpc_slurm_job_acct_gather_type }}
JobAcctGatherFrequency={{ openhpc_slurm_job_acct_gather_frequency }}
#
AccountingStorageType={{ openhpc_slurm_accounting_storage_type }}
AccountingStorageHost={{ openhpc_slurm_accounting_storage_host }}
AccountingStoragePort={{ openhpc_slurm_accounting_storage_port }}
AccountingStorageLoc=slurm_acct_db
{% set storage_pass = openhpc_slurm_accounting_storage_pass | mandatory('You must set openhpc_slurm_accounting_storage_pass') if openhpc_slurm_accounting_storage_type in ['accounting_storage/mysql'] else '' %}
{%- if storage_pass -%}
AccountingStoragePass={{ storage_pass }}
{%- endif -%}
AccountingStorageUser={{ openhpc_slurm_accounting_storage_user }}
#
# COMPUTE NODES
# OpenHPC default configuration
PropagateResourceLimitsExcept=MEMLOCK
# By default, SLURM will log to syslog, which is what we want
#SlurmdLogFile=/var/log/slurm.log
#SlurmctldLogFile=/var/log/slurmctld.log
Epilog=/etc/slurm/slurm.epilog.clean
{% for part in openhpc_slurm_partitions %}
{% for group in part.get('groups', [part]) %}
{% set group_name = group.cluster_name|default(openhpc_cluster_name) ~ '_' ~ group.name %}
{# If using --limit, the first host in each group may not have facts available. Find one that does. #}
{% set group_hosts = groups[group_name] | intersect(play_hosts) %}

NodeName=DEFAULT State=UNKNOWN RealMemory={% if 'ram_mb' in group %}{{group.ram_mb}}{% else %}1{% endif %}{% if group_hosts | length > 0 %}\
{% set first_host_hv = hostvars[group_hosts | first] %}
    Sockets={{first_host_hv['ansible_processor_count']}} \
    CoresPerSocket={{first_host_hv['ansible_processor_cores']}} \
    ThreadsPerCore={{first_host_hv['ansible_processor_threads_per_core']}}
{% endif %}{# group_hosts has a length #}
{% for node in groups[group_name] %}
NodeName={{ node }}
{% endfor %}{# nodes #}
{% endfor %}{# group #}
PartitionName={{part.name}} \
    Default={% if 'default' in part %}{{ part.default }}{% else %}YES{% endif %} \
    MaxTime={% if 'maxtime' in part %}{{ part.maxtime }}{% else %}{{ openhpc_job_maxtime }}{% endif %} \
    State=UP \
    Nodes=\
{% for group in part.get('groups', [part]) %}
{% set group_name = group.cluster_name|default(openhpc_cluster_name) ~ '_' ~ group.name %}
{{ groups[group_name] | join(",\\\n") }}{% if not loop.last %},\
{% endif %}
{% endfor %}{# group #}
{% endfor %}{# partitions #}

# Want nodes that drop out of SLURM's configuration to be automatically
# returned to service when they come back.
ReturnToService=2
